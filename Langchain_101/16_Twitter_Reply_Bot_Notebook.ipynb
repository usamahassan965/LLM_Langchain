{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain"
      ],
      "metadata": {
        "id": "yE6VQhEE4wcI",
        "outputId": "0dd63633-1f45-445b-f44b-e448c951dc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "yE6VQhEE4wcI",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d336eed",
      "metadata": {
        "id": "6d336eed"
      },
      "source": [
        "# Twitter Reply Bot\n",
        "\n",
        "Ever see those Twitter bots that reply to comments automatically? Like this [one](https://twitter.com/explainthisbob/status/1661833808092471299?s=12) or [this one](https://twitter.com/replygpt/status/1661924851626696705?s=12)\n",
        "\n",
        "Let's create one ourselves. In this notebook we'll just look at the prompting technique I used. In the [full code](https://github.com/gkamradt/twitter-reply-bot) you'll see the other helper code to deploy this app.\n",
        "\n",
        "Here's how the final app will work\n",
        "\n",
        "1. A user @mentions your bot, for me it will be @SiliconOracle\n",
        "2. The script finds that new @mention and then reads the parent tweet it is being mentioned on\n",
        "3. The script takes that parent tweet and generates a witty response using a language model\n",
        "4. Respond is posted and tweet is logged\n",
        "\n",
        "This notebook will focus on #3.\n",
        "\n",
        "First let's import our packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c8ce8e2e",
      "metadata": {
        "id": "c8ce8e2e"
      },
      "outputs": [],
      "source": [
        "# from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate, SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "# from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# load_dotenv()\n",
        "\n",
        "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YourKey\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea87fe1",
      "metadata": {
        "id": "7ea87fe1"
      },
      "source": [
        "Then let's create our LLM, you should experiment with a larger 'temperature' since this is a creative task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "b004ffb5",
      "metadata": {
        "id": "b004ffb5"
      },
      "outputs": [],
      "source": [
        "# llm = ChatOpenAI(temperature=0.3,\n",
        "#                  openai_api_key=OPENAI_API_KEY,\n",
        "# #                  model_name='gpt-3.5-turbo',\n",
        "#                  model_name='gpt-4',\n",
        "#                 )\n",
        "# from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.llms import GooglePalm\n",
        "\n",
        "api_key = 'AIzaSyBTAOQXYm2EW_YDxIoV6eDV32L-_EegE0c'\n",
        "\n",
        "# llm = GoogleGenerativeAI(model= 'models/text-bison-001',google_api_key=api_key, temperature=0)\n",
        "llm = GooglePalm(google_api_key=api_key,temperature=0,max_output_tokens=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b757f875",
      "metadata": {
        "id": "b757f875"
      },
      "source": [
        "Then let's create our function that will take in a piece of a text (a tweet) and give us an output response\n",
        "\n",
        "> You are an incredibly wise and smart tech mad scientist from silicon valley.\n",
        "> Your goal is to give a concise prediction in response to a piece of text from the user. <br><br>\n",
        "\n",
        "> % RESPONSE TONE:\n",
        "> - Your prediction should be given in an active voice and be opinionated\n",
        "> - Your tone should be serious w/ a hint of wit and sarcasm\n",
        "\n",
        "> % RESPONSE FORMAT:\n",
        "> - Respond in under 200 characters\n",
        "> - Respond in one short sentence\n",
        "> - Do not respond with emojis\n",
        "\n",
        "> % RESPONSE CONTENT:\n",
        "> - Include specific examples of old tech if they are relevant\n",
        "> - If you don't have an answer, say, \"Sorry, my magic 8 ball isn't working right now 🔮\" <br><br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c7ecd314",
      "metadata": {
        "id": "c7ecd314"
      },
      "outputs": [],
      "source": [
        "def generate_response(llm, mentioned_parent_tweet_text):\n",
        "    # It would be nice to bring in information about the links, pictures, etc.\n",
        "    # But out of scope for now\n",
        "    system_template = \"\"\"\n",
        "        You are an incredibly wise and smart tech mad scientist from silicon valley.\n",
        "        Your goal is to give a concise prediction in response to a piece of text from the user.\n",
        "\n",
        "        % RESPONSE TONE:\n",
        "\n",
        "        - Your prediction should be given in an active voice and be opinionated\n",
        "        - Your tone should be serious w/ a hint of wit and sarcasm\n",
        "\n",
        "        % RESPONSE FORMAT:\n",
        "\n",
        "        - Respond in under 200 characters\n",
        "        - Respond in two or less short sentences\n",
        "        - Do not respond with emojis\n",
        "\n",
        "        % RESPONSE CONTENT:\n",
        "\n",
        "        - Include specific examples of old tech if they are relevant\n",
        "        - If you don't have an answer, say, \"Sorry, my magic 8 ball isn't working right now 🔮\"\n",
        "    \"\"\"\n",
        "    system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
        "\n",
        "    human_template=\"{text}\"\n",
        "    human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "    # get a chat completion from the formatted messages\n",
        "    final_prompt = chat_prompt.format_prompt(text=mentioned_parent_tweet_text).to_messages()\n",
        "    response = llm.invoke(final_prompt)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "35627f35",
      "metadata": {
        "id": "35627f35",
        "outputId": "1f199c61-57f1-4de7-830f-c89bb061830b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This bot will be the bane of your existence, but also a source of great amusement.\n"
          ]
        }
      ],
      "source": [
        "tweet = \"\"\"\n",
        "I wanted to build a sassy Twitter Bot that responded about the 'good ole days' of tech\n",
        "\n",
        "@SiliconOracle was built using @LangChainAI and hosted on @railway\n",
        "\n",
        "Condensed Prompt:\n",
        "You are a mad scientist from old school silicon valley that makes predictions on the future of a tweet\n",
        "\"\"\"\n",
        "\n",
        "response = generate_response(llm, tweet)\n",
        "print (response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1579a9dd",
      "metadata": {
        "id": "1579a9dd"
      },
      "source": [
        "Awesome, now that we have a prompt that we can respond to a tweet with, let's move onto deploying this code.\n",
        "\n",
        "Check out the full code [here](https://github.com/gkamradt/twitter-reply-bot)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}