{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "- Lanchain is a framework that allows you to build applications using LLMs."
      ],
      "metadata": {
        "id": "p-ufcM7b0Zar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai"
      ],
      "metadata": {
        "id": "FMu4AxBf2OEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fk-xO8kI0XiZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.6)"
      ],
      "metadata": {
        "id": "38bUnihr2pU9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Direct way of using llm"
      ],
      "metadata": {
        "id": "vNhoe6wB5PYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = llm('I want to open a restaurant for Pakistani food. Suggest a fancy name for this.')\n",
        "print(name)"
      ],
      "metadata": {
        "id": "ngDTt3fd3Sks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Prompt Template and LLMChains"
      ],
      "metadata": {
        "id": "0ST1obw05W0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate(input_variables=['cuisine'],template= 'I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.')\n",
        "\n",
        "#prompt_template.format(cuisine='Pakistani')  ## prompt_template is just a Python string"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y_DjlCHK3Evr",
        "outputId": "959c9e85-ddb8-445d-e415-5b1a23daaa90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I want to open a restaurant for Pakistani food. Suggest a fancy name for this.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "chain.run('Mexican')"
      ],
      "metadata": {
        "id": "yIYB7Vm_6XqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple Sequential Chain (Single Input-Output)\n",
        "\n",
        "- If you have multiple stages (chains) between an input and output then we use simple sequential chain"
      ],
      "metadata": {
        "id": "g6NG2pn17zIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "prompt_template_name = PromptTemplate(input_variables=['cuisine'],template= 'I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.')\n",
        "name_chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "\n",
        "prompt_template_items = PromptTemplate(input_variables=['restaurant_name'],template= 'Suggest some menu items for {restaurant_name}. Return it as a comma separated list.')\n",
        "food_chain = LLMChain(llm=llm,prompt=prompt_template_items)\n",
        "\n",
        "chain = SimpleSequentialChain(chains=[name_chain,food_chain])\n",
        "\n",
        "response = chain.run('Pakistani')\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "eAU7r0ki7x_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sequential Chain (Multi Inputs / Outputs)"
      ],
      "metadata": {
        "id": "3ld7e44a_ZI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "prompt_template_name = PromptTemplate(input_variables=['cuisine'],template= 'I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.')\n",
        "name_chain = LLMChain(llm=llm,prompt=prompt_template_name,output_key='restaurant_name')\n",
        "\n",
        "prompt_template_items = PromptTemplate(input_variables=['restaurant_name'],template= 'Suggest some menu items for {restaurant_name}. Return it as a comma separated list.')\n",
        "food_chain = LLMChain(llm=llm,prompt=prompt_template_items,output_key='menu_items')\n",
        "\n",
        "chain = SequentialChain(chains=[name_chain,food_chain],input_variables=['cuisine'],output_variables=['restaurant_name','menu_items'])\n",
        "\n",
        "response = chain({'cuisine': 'Pakistani'})\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "dBF-aAYY_-gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Buffer Memory\n",
        "\n",
        "- If you want to make chatbot , you generally require to save history of conversation for later use.\n",
        "- For this, it requires an element of memory and langchain provides this attribute."
      ],
      "metadata": {
        "id": "nEGIyrQ5u5TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
        "name = chain.run('Pakistani')\n",
        "\n",
        "print(chain.memory)\n",
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "id": "29zU8uYgpZWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Window Buffer Memory\n",
        "\n",
        "- As openapi has its own cost per token, thus, we need to limit our buffer memory.\n",
        "- For this, we use window buffer memory feature."
      ],
      "metadata": {
        "id": "7ubzoEquxOxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k = 1)  ### Saving one conversational exchange\n",
        "chain = ConversationChain(llm=llm, memory=memory)\n",
        "\n",
        "chain.run('Who won the first cricket cup?')"
      ],
      "metadata": {
        "id": "Zzkx6C8IpZqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run('What was happened in 1947?')"
      ],
      "metadata": {
        "id": "7IIZ1LkwyRzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.memory.buffer)"
      ],
      "metadata": {
        "id": "D-TLmAOayt8V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}